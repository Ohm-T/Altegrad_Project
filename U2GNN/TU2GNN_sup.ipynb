{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24349023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from torch import optim\n",
    "import scipy.sparse as sp\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5cb2169",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mFloatTensor(indices, values, shape)\n\u001b[0;32m     57\u001b[0m \u001b[39m# Load graphs\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m edge_per_graph, adj, features, edge_features \u001b[39m=\u001b[39m load_data() \n\u001b[0;32m     60\u001b[0m \u001b[39m# Normalize adjacency matrices\u001b[39;00m\n\u001b[0;32m     61\u001b[0m adj \u001b[39m=\u001b[39m [normalize_adjacency(A) \u001b[39mfor\u001b[39;00m A \u001b[39min\u001b[39;00m adj]\n",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m A \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mT\n\u001b[0;32m     14\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mloadtxt(\u001b[39m\"\u001b[39m\u001b[39m../data/PROT_node_attributes.txt\u001b[39m\u001b[39m\"\u001b[39m, delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m edge_attr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mloadtxt(\u001b[39m\"\u001b[39;49m\u001b[39m../data/PROT_edge_attributes.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, delimiter\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     17\u001b[0m adj \u001b[39m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m features \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\salem\\miniconda3\\envs\\data\\lib\\site-packages\\numpy\\lib\\npyio.py:1154\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m   1153\u001b[0m     chunk \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 1154\u001b[0m     \u001b[39mfor\u001b[39;00m lineno, words \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mislice(\n\u001b[0;32m   1155\u001b[0m             lineno_words_iter, _loadtxt_chunksize):\n\u001b[0;32m   1156\u001b[0m         \u001b[39mif\u001b[39;00m usecols_getter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1157\u001b[0m             words \u001b[39m=\u001b[39m usecols_getter(words)\n",
      "File \u001b[1;32mc:\\Users\\salem\\miniconda3\\envs\\data\\lib\\encodings\\cp1252.py:22\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mIncrementalDecoder\u001b[39;00m(codecs\u001b[39m.\u001b[39mIncrementalDecoder):\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     23\u001b[0m         \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39mcharmap_decode(\u001b[39minput\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#nnz ne fonctionne pas correctement\n",
    "#need to take diago superior of nnz because adj symmetric\n",
    "\n",
    "def load_data(): \n",
    "    \"\"\"\n",
    "    Function that loads graphs\n",
    "    \"\"\"  \n",
    "    graph_indicator = np.loadtxt(\"../data/PROT_graph_indicator.txt\", dtype=np.int64)\n",
    "    _, graph_size = np.unique(graph_indicator, return_counts=True)\n",
    "    \n",
    "    edges = np.loadtxt(\"../data/edgelist.txt\", dtype=np.int64, delimiter=\",\")\n",
    "    A = sp.csr_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(graph_indicator.size, graph_indicator.size))\n",
    "    A += A.T\n",
    "    x = np.loadtxt(\"../data/PROT_node_attributes.txt\", delimiter=\",\")\n",
    "    edge_attr = np.loadtxt(\"../data/PROT_edge_attributes.txt\", delimiter=\",\")\n",
    "    \n",
    "    adj = []\n",
    "    features = []\n",
    "    edge_features = []\n",
    "    edge_per_graph = []\n",
    "    idx_n = 0\n",
    "    idx_m = 0\n",
    "    for i in range(graph_size.size):\n",
    "        adj.append(A[idx_n:idx_n+graph_size[i],idx_n:idx_n+graph_size[i]])\n",
    "        edge_a_triu = len(np.where(adj[i][np.triu_indices(adj[i].shape[0])])[0]!=0)\n",
    "        edge_features.append(edge_attr[idx_m:idx_m+edge_a_triu,:])\n",
    "        features.append(x[idx_n:idx_n+graph_size[i],:])\n",
    "        edge_per_graph.append(edge_a_triu)\n",
    "        idx_n += graph_size[i]\n",
    "        idx_m += edge_a_triu\n",
    "\n",
    "    return edge_per_graph, adj, features, edge_features\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    \"\"\"\n",
    "    Function that normalizes an adjacency matrix\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    A = A + sp.identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D = sp.diags(inv_degs)\n",
    "    A_normalized = D.dot(A)\n",
    "\n",
    "    return A_normalized\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"\n",
    "    Function that converts a Scipy sparse matrix to a sparse Torch tensor\n",
    "    \"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "# Load graphs\n",
    "edge_per_graph, adj, features, edge_features = load_data() \n",
    "\n",
    "# Normalize adjacency matrices\n",
    "adj = [normalize_adjacency(A) for A in adj]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee00e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 2.35 9.87 7.0 6.11 89.0 4.0 11.5 0.0 8.1 4.34 78.0 0.62 -0.4 -0.5 1.8 12.97 0.44 0.616 0.61 0.31 0.1 0.3 5.33 1.36 0.39 0.62 1.94 1.15 -0.3 2.1 0.42 0.35 5.1 3.9 7.3 0.38 -0.1 0.5 11.2 6.6 0.38 0.74 0.0 86.599998 0.36 1.42 0.83 0.66 1.489 0.709 0.788 0.824 1.29 0.9 0.77 0.92 0.9 1.0 8.3 8.25 100.0\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(list(map(str,features[0][30][23:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d6af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into training and test sets\n",
    "# adj_train = list()\n",
    "# features_train = list()\n",
    "# y_train = list()\n",
    "# # adj_test = list()\n",
    "# # features_test = list()\n",
    "# proteins_test = list()\n",
    "# with open('data/graph_labels.txt', 'r') as f:\n",
    "#     for i,line in enumerate(f):\n",
    "#         t = line.split(',')\n",
    "#         if len(t[1][:-1]) == 0:\n",
    "#             proteins_test.append(t[0])\n",
    "#         #             adj_test.append(adj[i])\n",
    "#         #             features_test.append(features[i])\n",
    "#         else:\n",
    "#         #             adj_train.append(adj[i])\n",
    "#         #             features_train.append(features[i])\n",
    "#             y_train.append(int(t[1][:-1]))\n",
    "    \n",
    "# print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f9e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = nx.Graph(adj[0], nodetype=int)\n",
    "# print(G.number_of_nodes())\n",
    "# print(adj[0].shape)\n",
    "# nnei = [str(x) for x in G.neighbors(1)]\n",
    "# print(nnei)\n",
    "# print(' '.join(nnei))\n",
    "# r = [1]\n",
    "# print(r, int(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028541a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n"
     ]
    }
   ],
   "source": [
    "fft = features[824]\n",
    "for i,x in enumerate(fft):\n",
    "    if len(np.where(x[3:23]==1)[0])==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1afada9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb23bf3448e4f25897d302aacf46845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[38;5;66;03m#{' '.join(list(map(str, feature_train[23:])))}\u001b[39;00m\n\u001b[1;32m     32\u001b[0m path_graph_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/graph_labels.txt\u001b[39m\u001b[38;5;124m'\u001b[39m                     \n\u001b[0;32m---> 33\u001b[0m \u001b[43mdata_text_tranformers_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_graph_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [13], line 29\u001b[0m, in \u001b[0;36mdata_text_tranformers_file\u001b[0;34m(path_graph_label)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     id_node \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(feature_train[\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m23\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/format_dataset_model.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_node[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_neighbors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(neighbors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/codecs.py:186\u001b[0m, in \u001b[0;36mIncrementalEncoder.__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIncrementalEncoder\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    An IncrementalEncoder encodes an input in multiple steps. The input can\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    be passed piece by piece to the encode() method. The IncrementalEncoder\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    remembers the state of the encoding process between calls to encode().\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m        Creates an IncrementalEncoder instance.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m        for a list of possible values.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors \u001b[38;5;241m=\u001b[39m errors\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def data_text_tranformers_file(path_graph_label):\n",
    "    with open('../data/format_dataset_model.txt', 'w') as f:\n",
    "        f.write('4888\\n')\n",
    "    with open(path_graph_label, 'r') as f:\n",
    "        proteins_test = list()\n",
    "        adj_test = list()\n",
    "        features_test = list()\n",
    "        for i,line in enumerate(tqdm(f, total=6111)):\n",
    "            t = line.split(',')\n",
    "            if len(t[1][:-1]) == 0:\n",
    "                proteins_test.append(t[0])\n",
    "                adj_test.append(adj[i])\n",
    "                features_test.append(features[i])\n",
    "            else:\n",
    "                adjacency, label = adj[i], int(t[1][:-1])\n",
    "                G = nx.Graph(adjacency, nodetype=int)\n",
    "                n_nodes = G.number_of_nodes()\n",
    "                with open('../data/format_dataset_model.txt', 'a') as f:\n",
    "                    f.write(f'{n_nodes} {label}\\n')\n",
    "                \n",
    "                for j in range(n_nodes):\n",
    "                    neighbors = [str(x) for x in G.neighbors(j)]\n",
    "                    n_neighbors = len(neighbors)\n",
    "                    feature_train = features[i][j]\n",
    "                    if len(np.where(feature_train[3:23]==1))==0:\n",
    "                        id_node = 0\n",
    "                    else:\n",
    "                        id_node = np.where(feature_train[3:23]==1)[0]\n",
    "                    open('../data/format_dataset_model.txt', 'a').write(f\"{id_node[0]} {n_neighbors} {' '.join(neighbors)}\\n\")\n",
    "                        #{' '.join(list(map(str, feature_train[23:])))}\n",
    "                        \n",
    "path_graph_label = '../data/graph_labels.txt'                     \n",
    "data_text_tranformers_file(path_graph_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73f10a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:53) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "497396fc279c3f97c33f1484ac27517a9d57f6ae267a9924b25221b965319414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
