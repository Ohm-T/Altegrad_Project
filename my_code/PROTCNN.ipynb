{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d72603ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# import sys\n",
    "import json\n",
    "import operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffe961b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda\n"
     ]
    }
   ],
   "source": [
    "path_root = ''\n",
    "path_to_data = path_root + 'data/'\n",
    "\n",
    "d = 20 # dimensionality of amino acid embeddings\n",
    "n_units = 100 # RNN layer dimensionality\n",
    "drop_rate = 0.3 # dropout\n",
    "input_size = (4888, 989, 20)\n",
    "\n",
    "padding_idx = 0\n",
    "oov_idx = 1\n",
    "batch_size = 32\n",
    "nb_epochs = 10\n",
    "my_patience = 2 # for early stopping strategy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bd8fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files for ohe\n",
    "graph_indicator = np.loadtxt(\"graph_indicator.txt\", dtype=np.int64)\n",
    "nodes = np.loadtxt(\"node_attributes.txt\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d0c4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "def create_dicts(sequence=amino_acids):\n",
    "    \"\"\"\n",
    "    Create the dicts for the sequence embedding\n",
    "    \"\"\"\n",
    "    word_to_index = dict(zip(sequence, range(1,21)))\n",
    "    # invert mapping\n",
    "    index_to_word =  {v : k for k, v in word_to_index.items()}\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "word_to_index, index_to_word = create_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ee0aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sequences\n",
    "sequences = list()\n",
    "with open('sequences.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        sequences.append(line[:-1])\n",
    "\n",
    "# Split data into training and test sets\n",
    "sequences_train = list()\n",
    "sequences_test = list()\n",
    "train_ohe = list()\n",
    "test_ohe = list()\n",
    "proteins_test = list()\n",
    "y_train = list()\n",
    "with open('graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        ohe_vec = torch.Tensor([node[3:23] for node in nodes[np.where(graph_indicator==i)]])\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            sequences_test.append(sequences[i])\n",
    "            test_ohe.append(ohe_vec)\n",
    "            \n",
    "        else:\n",
    "            sequences_train.append(sequences[i])\n",
    "            y_train.append(int(t[1][:-1]))\n",
    "            train_ohe.append(ohe_vec)\n",
    "\n",
    "\n",
    "\n",
    "train_ohe = pad_sequence(train_ohe).permute(1, 0, 2).long()\n",
    "test_ohe = pad_sequence(test_ohe).permute(1, 0, 2).long()\n",
    "pad_ = (0, 0, 0, 79)\n",
    "test_ohe = F.pad(test_ohe, pad_, \"constant\", 0)\n",
    "y_train = F.one_hot(torch.Tensor(y_train).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13858262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36]) 100\n",
      "torch.Size([50, 100])\n"
     ]
    }
   ],
   "source": [
    "# Desired max length\n",
    "max_len = 50\n",
    "\n",
    "# 100 seqs of variable length (< max_len)\n",
    "seq_lens = torch.randint(low=10,high=44,size=(100,))\n",
    "seqs = [torch.rand(n) for n in seq_lens]\n",
    "print(seqs[0].size(), len(seqs))\n",
    "# pad first seq to desired length\n",
    "seqs[0] = nn.ConstantPad1d((0, max_len - seqs[0].shape[0]), 0)(seqs[0])\n",
    "\n",
    "# pad all seqs to desired length\n",
    "seqs = pad_sequence(seqs)\n",
    "print(seqs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6e8600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class Dataset_(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.documents = x\n",
    "        self.labels = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        document = self.documents[index]\n",
    "        label = self.labels[index] \n",
    "        sample = {\n",
    "            \"document\": torch.tensor(document),\n",
    "            \"label\": torch.tensor(label),\n",
    "            }\n",
    "        return sample\n",
    "\n",
    "\n",
    "def get_loader(x, y, batch_size=32):\n",
    "    dataset = Dataset_(x, y)\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=True,\n",
    "                            )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5c62910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtCNN(nn.Module):    \n",
    "    def __init__(self, index_to_word, embed_dim, dropout=0.5):\n",
    "        super(ProtCNN, self).__init__()\n",
    "#         self.embedding = nn.Embedding(num_embeddings=len(index_to_word)+2,\n",
    "#                                           embedding_dim=d)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv1 = nn.Conv1d(in_channels=989, out_channels=128, kernel_size=1, stride=1, dilation=1, padding='same')\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=1, stride=1, dilation=2, padding='same')\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, stride=1, dilation=3, padding='same')\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size=3, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(128, 18)\n",
    "        \n",
    "        \n",
    "    def residual_block(self, x_in):\n",
    "        \"\"\"\n",
    "        _data: input\n",
    "        _filters: convolution filters\n",
    "        _d_rate: dilation rate\n",
    "        \"\"\"\n",
    "\n",
    "        shortcut = x_in\n",
    "        \n",
    "        x = self.bn1(x_in)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        #bottleneck convolution\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.conv3(x)\n",
    "\n",
    "        #skip connection\n",
    "        out += shortcut\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "#       x = self.embedding(x_in)\n",
    "\n",
    "        x = self.conv1(x_in)\n",
    "        x = self.residual_block(x)\n",
    "        print(x.size())\n",
    "        #x = residual_block(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        print(x.size())\n",
    "        out = self.dropout(x)\n",
    "       \n",
    "        # softmax classifier\n",
    "        out =  F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e26157e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 4)\n",
    "b = torch.ones(4, 4)\n",
    "c = torch.ones(7, 4)\n",
    "print(a, b, c)\n",
    "print(pad_sequence([a, b, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fda51706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/152 [00:00<?, ?batch/s]/tmp/ipykernel_105000/3703619249.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"document\": torch.tensor(document),\n",
      "/tmp/ipykernel_105000/3703619249.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"label\": torch.tensor(label),\n",
      "Epoch 1:   0%|                                                                               | 0/152 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 20])\n",
      "torch.Size([32, 128, 6])\n",
      "torch.Size([128, 6]) torch.Size([32, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (128) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 61\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#         else:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#             p += 1\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#             if p==my_patience:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#     model.load_state_dict(torch.load('./best_model.pt'))\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#     model.eval()\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [47], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(x_train, y_train, x_test, word_dict, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28minput\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39msize(), label\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 32\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# fill the gap # compute the loss\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;66;03m# prevent exploding gradient \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (128) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model = ProtCNN(index_to_word, n_units).to(device)\n",
    "model = model.double()\n",
    "lr = 0.001  # learning rate\n",
    "criterion = nn.CrossEntropyLoss()# fill the gap, use Binary cross entropy from torch.nn: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) #fill me\n",
    "\n",
    "def train(x_train=train_ohe,\n",
    "          y_train=y_train,\n",
    "          x_test=test_ohe,\n",
    "          word_dict=index_to_word,\n",
    "          batch_size=batch_size):\n",
    "  \n",
    "    train_data = get_loader(x_train, y_train, batch_size)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    p = 0 # patience\n",
    "\n",
    "    for epoch in range(1, nb_epochs + 1): \n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        with tqdm(train_data, unit=\"batch\") as tepoch:\n",
    "            for idx, data in enumerate(tepoch):\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                input = data['document'].to(device, dtype=torch.double)\n",
    "                label = data['label'].to(device)\n",
    "                label = label.double()\n",
    "                output = model.forward(input)[0]\n",
    "                print(output.size(), label.size())\n",
    "                loss = criterion(output, label) # fill the gap # compute the loss\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient \n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "#                 accuracy = torch.sum(torch.round(output) == label).item() / batch_size\n",
    "#                 accuracies.append(accuracy)\n",
    "#                 tepoch.set_postfix(loss=sum(losses)/len(losses), accuracy=100. * sum(accuracies)/len(accuracies))\n",
    "\n",
    "#         train_acc = evaluate_accuracy(train_data, False)\n",
    "        #test_acc = evaluate_accuracy(test_data, False)\n",
    "        print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\"\n",
    "              .format(epoch, sum(losses)/len(losses)))\n",
    "        train_loss = sum(losses)/len(losses)\n",
    "        if train_loss <= best_loss:\n",
    "            best_loss = train_loss\n",
    "            print(\"Train Loss improved, saving model...\")\n",
    "            torch.save(model.state_dict(), './best_model.pt')\n",
    "            p = 0\n",
    "#         else:\n",
    "#             p += 1\n",
    "#             if p==my_patience:\n",
    "#                 print(\"Validation accuracy did not improve for {} epochs, stopping training...\".format(my_patience))\n",
    "#     print(\"Loading best checkpoint...\")    \n",
    "#     model.load_state_dict(torch.load('./best_model.pt'))\n",
    "#     model.eval()\n",
    "    print('done.')\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432e74c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
