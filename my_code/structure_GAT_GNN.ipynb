{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca1fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e17107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(): \n",
    "    \"\"\"\n",
    "    Function that loads graphs\n",
    "    \"\"\"  \n",
    "    graph_indicator = np.loadtxt(\"graph_indicator.txt\", dtype=np.int64)\n",
    "    _,graph_size = np.unique(graph_indicator, return_counts=True)\n",
    "    \n",
    "    edges = np.loadtxt(\"edgelist.txt\", dtype=np.int64, delimiter=\",\")\n",
    "    A = sp.csr_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(graph_indicator.size, graph_indicator.size))\n",
    "    A += A.T\n",
    "    \n",
    "    x = np.loadtxt(\"node_attributes.txt\", delimiter=\",\")\n",
    "    edge_attr = np.loadtxt(\"edge_attributes.txt\", delimiter=\",\")\n",
    "    \n",
    "    adj = []\n",
    "    features = []\n",
    "    edge_features = []\n",
    "    idx_n = 0\n",
    "    idx_m = 0\n",
    "    for i in range(graph_size.size):\n",
    "        adj.append(A[idx_n:idx_n+graph_size[i],idx_n:idx_n+graph_size[i]])\n",
    "        edge_features.append(edge_attr[idx_m:idx_m+adj[i].nnz,:])\n",
    "        features.append(x[idx_n:idx_n+graph_size[i],:])\n",
    "        idx_n += graph_size[i]\n",
    "        idx_m += adj[i].nnz\n",
    "\n",
    "    return adj, features, edge_features\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    \"\"\"\n",
    "    Function that normalizes an adjacency matrix\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    A = A + sp.identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D = sp.diags(inv_degs)\n",
    "    A_normalized = D.dot(A)\n",
    "\n",
    "    return A_normalized\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"\n",
    "    Function that converts a Scipy sparse matrix to a sparse Torch tensor\n",
    "    \"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bbc4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"GAT layer\"\"\"\n",
    "    def __init__(self, n_feat, n_hidden, alpha=0.05):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.fc = nn.Linear(n_feat, n_hidden, bias=False)\n",
    "        self.a = nn.Linear(2*n_hidden, 1)\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \n",
    "        ############## Task 1\n",
    "    \n",
    "        ##################\n",
    "        indices = adj.coalesce().indices()\n",
    "        z = self.fc(x)\n",
    "        h = torch.cat((z[indices[0,:], :], z[indices[1,:], :]), dim=1)\n",
    "        h = self.a(h)\n",
    "        h = self.leakyrelu(h)\n",
    "        ##################\n",
    "\n",
    "        h = torch.exp(h.squeeze())\n",
    "        unique = torch.unique(indices[0,:])\n",
    "        t = torch.zeros(unique.size(0), device=x.device)\n",
    "        h_sum = t.scatter_add(0, indices[0,:], h)\n",
    "        h_norm = torch.gather(h_sum, 0, indices[0,:])\n",
    "        alpha = torch.div(h, h_norm)\n",
    "        adj_att = torch.sparse.FloatTensor(indices, alpha, torch.Size([x.size(0), x.size(0)])).to(x.device)\n",
    "        \n",
    "        ##################\n",
    "        out = torch.sparse.mm(adj_att, z)\n",
    "        ##################\n",
    "\n",
    "        return out, alpha\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    \"\"\"GNN model\"\"\"\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.mp1 = GATLayer(nfeat, nhid)\n",
    "        self.mp2 = GATLayer(nhid, nhid)\n",
    "        self.fc = nn.Linear(nhid, nclass)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \n",
    "        ############## Tasks 2 and 4\n",
    "    \n",
    "        ##################\n",
    "\n",
    "        # second message passing layer\n",
    "        h1, _ = self.mp1(x, adj)\n",
    "        h1 = self.relu(h1)\n",
    "        h1_drop = self.dropout(h1)\n",
    "\n",
    "        # second message passing layer\n",
    "        h2, alpha = self.mp2(h1_drop, adj)\n",
    "        h2 = self.relu(h2)\n",
    "\n",
    "        out = self.fc(h2)\n",
    "\n",
    "        ##################\n",
    "\n",
    "        return F.log_softmax(out, dim=1), alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7deddfdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load graphs\n",
    "adj, features, edge_features = load_data() \n",
    "\n",
    "# Normalize adjacency matrices\n",
    "adj = [normalize_adjacency(A) for A in adj]\n",
    "\n",
    "# Split data into training and test sets\n",
    "adj_train = list()\n",
    "features_train = list()\n",
    "y_train = list()\n",
    "adj_test = list()\n",
    "features_test = list()\n",
    "proteins_test = list()\n",
    "with open('graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            adj_test.append(adj[i])\n",
    "            features_test.append(features[i])\n",
    "        else:\n",
    "            adj_train.append(adj[i])\n",
    "            features_train.append(features[i])\n",
    "            y_train.append(int(t[1][:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e211e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4888\n",
      "(162, 86)\n",
      "4888 (162, 162)\n"
     ]
    }
   ],
   "source": [
    "print(len(features_train))\n",
    "print(features_train[1].shape)\n",
    "print(len(adj_train), adj_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00009810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8221, 18]) tensor([ 8,  4,  8,  8, 15,  8,  2,  5, 14,  8,  6,  0,  2,  7, 15,  5,  4,  6,\n",
      "         2, 11,  8,  8,  6,  2,  2,  2,  0,  2,  8,  8,  8,  2],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (8221) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m model(features_batch, adj_batch)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39msize(), y_batch)\n\u001b[0;32m---> 54\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     56\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (8221) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "# Initialize device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 200\n",
    "n_hidden = 16\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.1\n",
    "n_class = 18\n",
    "batch_size=32\n",
    "# Compute number of training and test samples\n",
    "N_train = len(adj_train)\n",
    "N_test = len(adj_test)\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(86, n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    # Iterate over the batches\n",
    "    for i in range(0, N_train, batch_size):\n",
    "        adj_batch = list()\n",
    "        features_batch = list()\n",
    "        idx_batch = list()\n",
    "        y_batch = list()\n",
    "    \n",
    "        # Create tensors\n",
    "        for j in range(i, min(N_train, i+batch_size)):\n",
    "            n = adj_train[j].shape[0]\n",
    "            adj_batch.append(adj_train[j]+sp.identity(n))\n",
    "            features_batch.append(features_train[j])\n",
    "            idx_batch.extend([j-i]*n)\n",
    "            y_batch.append(y_train[j])\n",
    "\n",
    "        adj_batch = sp.block_diag(adj_batch)\n",
    "        features_batch = np.vstack(features_batch)\n",
    "\n",
    "        adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch).to(device)\n",
    "        features_batch = torch.FloatTensor(features_batch).to(device)\n",
    "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "        y_batch = torch.LongTensor(y_batch).to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(features_batch, adj_batch)\n",
    "        print(output.size(), y_batch)\n",
    "        loss_train = loss_function(output, y_batch)\n",
    "        train_loss += loss.item() * output.size(0)\n",
    "        count += output.size(0)\n",
    "        preds = output.max(1)[1].type_as(y_batch)\n",
    "        correct += torch.sum(preds.eq(y_batch).double())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch: {:03d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Testing\n",
    "# model.eval()\n",
    "# output, alpha = model(features, adj)\n",
    "# loss_test = F.nll_loss(output[idx_test], y[idx_test])\n",
    "# acc_test = accuracy_score(torch.argmax(output[idx_test], dim=1).detach().cpu().numpy(), y[idx_test].cpu().numpy())\n",
    "# print(\"Test set results:\",\n",
    "#       \"loss= {:.4f}\".format(loss_test.item()),\n",
    "#       \"accuracy= {:.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f3b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
