{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e94df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Read sequences\n",
    "sequences = list()\n",
    "with open('sequences.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        sequences.append(line[:-1])\n",
    "\n",
    "# Split data into training and test sets\n",
    "sequences_train = list()\n",
    "sequences_test = list()\n",
    "proteins_test = list()\n",
    "y_train = list()\n",
    "with open('graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            sequences_test.append(sequences[i])\n",
    "        else:\n",
    "            sequences_train.append(sequences[i])\n",
    "            y_train.append(int(t[1][:-1]))\n",
    "\n",
    "# Map sequences to \n",
    "vec = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "X_train = vec.fit_transform(sequences_train)\n",
    "X_test = vec.transform(sequences_test)\n",
    "\n",
    "# Train a logistic regression classifier and use the classifier to\n",
    "# make predictions\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_test)\n",
    "\n",
    "# Write predictions to a file\n",
    "with open('sample_submission_seq.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = list()\n",
    "    for i in range(18):\n",
    "        lst.append('class'+str(i))\n",
    "    lst.insert(0, \"name\")\n",
    "    writer.writerow(lst)\n",
    "    for i, protein in enumerate(proteins_test):\n",
    "        lst = y_pred_proba[i,:].tolist()\n",
    "        lst.insert(0, protein)\n",
    "        writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60f6397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4888, 18)\n",
      "(1223, 18)\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict_proba(X_train).shape)\n",
    "print(y_pred_proba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aba705ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "# Read sequences\n",
    "sequences = list()\n",
    "with open('sequences.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        sequences.append(line[:-1])\n",
    "\n",
    "# Split data into training and test sets\n",
    "sequences_train = list()\n",
    "sequences_test = list()\n",
    "proteins_test = list()\n",
    "y_train = list()\n",
    "with open('graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            sequences_test.append(sequences[i])\n",
    "        else:\n",
    "            sequences_train.append(sequences[i])\n",
    "            y_train.append(int(t[1][:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7664e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"\n",
    "    Function that converts a Scipy sparse matrix to a sparse Torch tensor\n",
    "    \"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3204becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4888, 8466)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'todense'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(X_train))\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msequences_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtodense\u001b[49m())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m tensor \u001b[38;5;241m=\u001b[39m sparse_mx_to_torch_sparse_tensor(X_train)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'todense'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.shape(X_train))\n",
    "print(sequences_train[0].todense())\n",
    "print(X_train[0])\n",
    "tensor = sparse_mx_to_torch_sparse_tensor(X_train)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff7c7a",
   "metadata": {},
   "source": [
    "# First approach encoding with one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4776f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7aea69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ProtCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, batch_size, dropout=0.5):\n",
    "        super(ProtCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                          embedding_dim=embed_dim,\n",
    "                                          padding_idx=0,\n",
    "                                          max_norm=5.0)\n",
    "        self.bn1 = nn.BatchNorm1d(1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=128, kernel_size=1, stride=1, dilation=1, padding='same')\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=1, stride=1, dilation=2, padding='same')\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, stride=1, dilation=3, padding='same')\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size=3, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(128, 18)\n",
    "        \n",
    "        \n",
    "    def residual_block(self, x_in):\n",
    "        \"\"\"\n",
    "        _data: input\n",
    "        _filters: convolution filters\n",
    "        _d_rate: dilation rate\n",
    "        \"\"\"\n",
    "\n",
    "        shortcut = x_in\n",
    "        \n",
    "        x = self.bn1(x_in)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        #bottleneck convolution\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.conv3(x)\n",
    "\n",
    "        #skip connection\n",
    "        out += shortcut\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        x = self.embedding(x_in)\n",
    "        x_reshaped = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x_reshaped)\n",
    "        x = residual_block(x)\n",
    "        #x = residual_block(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        out = self.dropout(x)\n",
    "       \n",
    "        # softmax classifier\n",
    "        out =  F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c029ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Extract dataset\n",
    "\n",
    "# Read sequences\n",
    "sequences = list()\n",
    "with open('sequences.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        sequences.append(line[:-1])\n",
    "\n",
    "# Split data into training and test sets\n",
    "sequences_train = list()\n",
    "sequences_test = list()\n",
    "proteins_test = list()\n",
    "y_train = list()\n",
    "with open('graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            sequences_test.append(sequences[i])\n",
    "        else:\n",
    "            sequences_train.append(sequences[i])\n",
    "            y_train.append(int(t[1][:-1]))\n",
    "\n",
    "# Map sequences to \n",
    "vec = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "X_train = vec.fit_transform(sequences_train)\n",
    "X_test = vec.transform(sequences_test)\n",
    "\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]\n",
    "\n",
    "# Initializes model and optimizer\n",
    "model = ProtCNN(batch_size, dropout=0.5).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26f3e3aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 1, 1], expected input[1, 64, 8466] to have 1 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(y_train[i:\u001b[38;5;28mmin\u001b[39m(N_train, i\u001b[38;5;241m+\u001b[39mbatch_size)])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39msize(), y_batch\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, y_batch)\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [5], line 44\u001b[0m, in \u001b[0;36mProtCNN.forward\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_in):\n\u001b[0;32m---> 44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     x \u001b[38;5;241m=\u001b[39m residual_block(x)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m#x = residual_block(x)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 1, 1], expected input[1, 64, 8466] to have 1 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "# Eval\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for i in range(0, N_train, batch_size):\n",
    "\n",
    "        X_batch = X_train[i:min(N_train, i+batch_size)]\n",
    "        X_batch = sparse_mx_to_torch_sparse_tensor(X_batch).to(device)\n",
    "        y_batch = torch.LongTensor(y_train[i:min(N_train, i+batch_size)]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        print(output.size(), y_batch.size())\n",
    "        loss = loss_function(output, y_batch)\n",
    "        train_loss += loss.item() * output.size(0)\n",
    "        count += output.size(0)\n",
    "        preds = output.max(1)[1].type_as(y_batch)\n",
    "        correct += torch.sum(preds.eq(y_batch).double())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(train_loss / count),\n",
    "              'acc_train: {:.4f}'.format(correct / count),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "        \n",
    "print('Optimization finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "for i in range(0, N_test, batch_size):\n",
    "    adj_batch = list()\n",
    "    idx_batch = list()\n",
    "    y_batch = list()\n",
    "\n",
    "    ############## Task 7\n",
    "    \n",
    "    ##################\n",
    "    g_batch = G_test[i:i+batch_size]\n",
    "    adj_batch = sp.block_diag([nx.adjacency_matrix(g) for g in g_batch])\n",
    "    adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch)\n",
    "\n",
    "    for j, G in enumerate(g_batch):\n",
    "        idx_batch.extend(G.number_of_nodes()*[j])\n",
    "    idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "    features_batch = torch.ones((len(idx_batch), 1)).to(device)\n",
    "    y_batch = torch.LongTensor(y_test[i:i+batch_size]).to(device)\n",
    "    ##################\n",
    "\n",
    "    output = model(features_batch, adj_batch, idx_batch)\n",
    "    loss = loss_function(output, y_batch)\n",
    "    test_loss += loss.item() * output.size(0)\n",
    "    count += output.size(0)\n",
    "    preds = output.max(1)[1].type_as(y_batch)\n",
    "    correct += torch.sum(preds.eq(y_batch).double())\n",
    "\n",
    "print('loss_test: {:.4f}'.format(test_loss / count),\n",
    "      'acc_test: {:.4f}'.format(correct / count),\n",
    "      'time: {:.4f}s'.format(time.time() - t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896fbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
