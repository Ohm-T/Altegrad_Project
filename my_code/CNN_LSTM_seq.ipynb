{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898e89ae",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece4d177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 08:21:01.444312: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3768dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sequences\n",
    "sequences = list()\n",
    "with open(os.path.join('data', 'sequences.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        sequences.append(line[:-1])\n",
    "\n",
    "# Split data into training and test sets\n",
    "sequences_train = list()\n",
    "sequences_test = list()\n",
    "proteins_test = list()\n",
    "y_train = list()\n",
    "with open(os.path.join('data', 'graph_labels.txt'), 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            sequences_test.append(sequences[i])\n",
    "        else:\n",
    "            sequences_train.append(sequences[i])\n",
    "            y_train.append(int(t[1][:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef799a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(zip(sequences_train, y_train), columns = ['Sequence', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd8030f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train.Label.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42cd26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4888, 600) X_test shape: (1224, 600) y_train shape: (4888, 18) y_test shape: torch.Size([1223, 18])\n"
     ]
    }
   ],
   "source": [
    "# Encode Labels\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(df_train['Label'])\n",
    "\n",
    "# Tokenize Sequence with keras\n",
    "max_len = 600\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "\n",
    "# Tokenise Sequence with TFIDF\n",
    "# vec = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "# X_train = vec.fit_transform(sequences_train)\n",
    "# X_test = vec.transform(sequences_test)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(sequences_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "# Not real y_test but y_test from initial model\n",
    "df_test = pd.read_csv('sample_submission/sample_submission_test.csv', sep=',')\n",
    "y_test = torch.LongTensor(df_test.iloc[:, 1:].to_numpy())\n",
    "\n",
    "\n",
    "# Deal with last batch size\n",
    "X_test = np.vstack((X_test, np.zeros((1, max_len))))\n",
    "X_train = tokenizer.texts_to_sequences(sequences_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "print('X_train shape:', X_train.shape, 'X_test shape:', X_test.shape, 'y_train shape:', y_train.shape, 'y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30153201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "\n",
    "class Sequence_Dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0554c44",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef42502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All parameters\n",
    "\n",
    "vocab_size = len(np.unique(X_train))\n",
    "sequence_length = max_len\n",
    "embedding_size = 21\n",
    "hidden_size = 512\n",
    "num_classes = 18 # number of different proteins\n",
    "lr = 0.001\n",
    "batch_size = 136\n",
    "dropout = 0.7\n",
    "n_epochs = 30\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.get_device_name()\n",
    "\n",
    "# BiLSTM Params\n",
    "num_layers = 3 # number of layers for BiLTSM model\n",
    "\n",
    "# CNN Params\n",
    "n_filters = 300\n",
    "filter_sizes = [3, 10, 20, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5791438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (embedding): Embedding(22, 21)\n",
      "  (lstm): LSTM(21, 512, num_layers=3, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers, num_classes, batch_size):\n",
    "        \"\"\"\n",
    "        vocab_size: int, number of words in vocbulary\n",
    "        emedding_size: int, embedding dimension\n",
    "        hidden_size: int, size of hidden layer\n",
    "        num_layers: int, number of LSTM layers\n",
    "        num_classes: number of classes\n",
    "        batch_size: size of mini batches\n",
    "        \"\"\"\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        # self.hidden = self.init_hidden()\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "  \n",
    "    def init_hidden(self):\n",
    "        # initialise hidden & cell state\n",
    "        h0 = Variable(torch.zeros(self.num_layers*2, self.batch_size, self.hidden_size)).to(device)\n",
    "        c0 = Variable(torch.zeros(self.num_layers*2, self.batch_size, self.hidden_size)).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        x = self.embedding(inputs) # x=[batch_size, sequence_len, embedding_size]\n",
    "        # lstm_out, self.hidden = self.lstm(x, self.hidden) # lstm_out=[batch_size, sequence_len, hidden_size]\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.permute(1,0,2) # lstm_out=[sequence_len, batch_size, hidden_size]\n",
    "        y = self.fc1(lstm_out[-1]) # take last hidden state, y=[batch_size, num_classes]\n",
    "        y = F.leaky_relu(y, 0.1)\n",
    "        y = self.dropout(y)\n",
    "        y = self.fc2(y)\n",
    "        probs = F.log_softmax(y, dim=1) # probs=[batch_size, num_classes]\n",
    "        return probs\n",
    "\n",
    "# generate model instance\n",
    "bilstm_model = BiLSTM(vocab_size, embedding_size, hidden_size, num_layers, num_classes, batch_size).to(device)\n",
    "print(bilstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97521d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embedding): Embedding(22, 21)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 300, kernel_size=(3, 21), stride=(1, 1))\n",
      "    (1): Conv2d(1, 300, kernel_size=(10, 21), stride=(1, 1))\n",
      "    (2): Conv2d(1, 300, kernel_size=(20, 21), stride=(1, 1))\n",
      "    (3): Conv2d(1, 300, kernel_size=(40, 21), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=1200, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    vocab_size: int, number of words in vocbulary\n",
    "    emedding_size: int, embedding dimension\n",
    "    hidden_size: int, size of hidden layer\n",
    "    n_filters: int, number of filters to use per conv layer\n",
    "    filter_sizes: list(int), list of conv filter sizes\n",
    "    batch_size: size of mini batches\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, n_filters, filter_sizes, num_classes):\n",
    "        super(CNN, self).__init__()        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_size)) for fs in filter_sizes]).to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(filter_sizes)*n_filters, hidden_size, bias=True)  \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embedding(inputs) # x=[batch_size, seq_len, embedding_size]\n",
    "        x = x.unsqueeze(1) # x=[batch_size, 1, seq_len, embedding_size]  \n",
    "        conv_out = [F.relu(conv(x)).squeeze(3) for conv in self.convs] # conv_n = [batch size, n_filters, seq_len - filter_sizes[n]]\n",
    "        pool_out = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conv_out] # pool_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pool_out, dim = 1)) # cat = [batch size, len(filter_sizes)*n_filters]\n",
    "        y = self.fc1(F.relu(cat))\n",
    "        y = self.dropout(y)\n",
    "        y = self.fc2(y)\n",
    "        probs = F.log_softmax(y, dim=1) # probs=[batch_size, num_classes]\n",
    "        return probs\n",
    "\n",
    "cnn_model = CNN(vocab_size, embedding_size, hidden_size, n_filters, filter_sizes, num_classes).to(device)\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ae2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, n_filters, filter_sizes, num_layers, num_classes, batch_size, dropout):\n",
    "        \"\"\"\n",
    "        vocab_size: int, number of words in vocbulary\n",
    "        emedding_size: int, embedding dimension\n",
    "        hidden_size: int, size of hidden layer\n",
    "        num_layers: int, number of LSTM layers\n",
    "        num_classes: number of classes\n",
    "        batch_size: size of mini batches\n",
    "        \"\"\"\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_size)) for fs in filter_sizes]).to(device)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        l = n_filters*len(filter_sizes) + hidden_size*2\n",
    "        self.fc1 = nn.Linear(l, l//2)\n",
    "        self.fc2 = nn.Linear(l//2, num_classes)\n",
    "  \n",
    "    def init_hidden(self):\n",
    "        # initialise hidden & cell state\n",
    "        h0 = Variable(torch.zeros(self.num_layers*2, self.batch_size, self.hidden_size)).to(device)\n",
    "        c0 = Variable(torch.zeros(self.num_layers*2, self.batch_size, self.hidden_size)).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        x = self.embedding(inputs) # x=[batch_size, seq_len, embedding_size]\n",
    "\n",
    "        cnn_x = x.unsqueeze(1) # x=[batch_size, 1, seq_len, embedding_size]  \n",
    "        cnn_x = [F.relu(conv(cnn_x)).squeeze(3) for conv in self.convs] # conv_n = [batch size, n_filters, seq_len - filter_sizes[n]]\n",
    "        cnn_x = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in cnn_x]\n",
    "        cnn_out = self.dropout(torch.cat(cnn_x, dim = 1))\n",
    "\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.permute(1,0,2) # lstm_out=[sequence_len, batch_size, hidden_size] because batch_first = True\n",
    "        # feature extraction 1 using maxpool1d over all states\n",
    "        # lstm_out = th.transpose(lstm_out, 0, 1)\n",
    "        # lstm_out = th.transpose(lstm_out, 1, 2)\n",
    "        # lstm_out = F.max_pool1d(lstm_out, lstm_out.size(2)).squeeze(2)\n",
    "        # feature extraction 2 taking last hidden state\n",
    "        lstm_out = lstm_out[-1]\n",
    "        lstm_out = F.relu(lstm_out)\n",
    "\n",
    "        # concatenate CNN and LSTM output before FC layers\n",
    "        cnn_out = torch.transpose(cnn_out, 0, 1)\n",
    "        lstm_out = torch.transpose(lstm_out, 0, 1)\n",
    "        cnn_lstm_out = torch.cat((cnn_out, lstm_out), 0)\n",
    "        cnn_lstm_out = torch.transpose(cnn_lstm_out, 0, 1)\n",
    "\n",
    "        # FC layers\n",
    "        y = F.relu(cnn_lstm_out)\n",
    "        y = self.fc1(y)\n",
    "        y = self.dropout(y)\n",
    "        y = F.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        probs = F.log_softmax(y, dim=1) # probs=[batch_size, num_classes]\n",
    "        return probs\n",
    "\n",
    "# generate model instance\n",
    "# cnnbilstm_model = CNN_BiLSTM(vocab_size, embedding_size, hidden_size, n_filters, filter_sizes, num_layers, num_classes, batch_size).to(device)\n",
    "# print(cnnbilstm_model)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca6f6e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c10474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch training loop\n",
    "def train(dataloader, model, optimizer, criterion, pred_func):\n",
    "    \"\"\"\n",
    "    Main function for training LSTM model and printing prompt\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 11\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (batch, label) in enumerate(dataloader):\n",
    "        batch, label = batch.to(device), label.to(device)\n",
    "        model.zero_grad()\n",
    "        model, pred = pred_func(model, batch)\n",
    "        label = torch.argmax(label, 1) # one_hot --> indexing\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (pred.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader), total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "def append_metrics(history, metrics):\n",
    "    \"\"\" append metrics dictionary to history dictionary \"\"\"\n",
    "    for key in metrics.keys():\n",
    "        history[key].append(metrics[key])\n",
    "    return history\n",
    "\n",
    "def LSTM_pred(model, batch):\n",
    "    \"\"\" LSTM initialisation and prediction for training loop \"\"\"\n",
    "    hidden = model.init_hidden() # initalise hidden states\n",
    "    pred = model(batch, hidden)\n",
    "    return model, pred\n",
    "\n",
    "def CNN_pred(model, batch):\n",
    "    \"\"\" CNN prediction for taining loop \"\"\"\n",
    "    pred = model(batch)\n",
    "    return model, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c240e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Sequence_Dataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "#model = BiLSTM(vocab_size, embedding_size, hidden_size, num_layers, num_classes, batch_size).to(device)\n",
    "model = CNN_BiLSTM(vocab_size, embedding_size, hidden_size, n_filters, filter_sizes, num_layers, num_classes, batch_size, dropout).to(device)\n",
    "pred_func = LSTM_pred\n",
    "name = model.__class__.__name__ # model name\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1., gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf224b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    11/   35 batches | accuracy    0.162\n",
      "| epoch   1 |    22/   35 batches | accuracy    0.172\n",
      "| epoch   1 |    33/   35 batches | accuracy    0.200\n",
      "| epoch   2 |    11/   35 batches | accuracy    0.206\n",
      "| epoch   2 |    22/   35 batches | accuracy    0.209\n",
      "| epoch   2 |    33/   35 batches | accuracy    0.219\n",
      "| epoch   3 |    11/   35 batches | accuracy    0.256\n",
      "| epoch   3 |    22/   35 batches | accuracy    0.258\n",
      "| epoch   3 |    33/   35 batches | accuracy    0.285\n",
      "| epoch   4 |    11/   35 batches | accuracy    0.278\n",
      "| epoch   4 |    22/   35 batches | accuracy    0.258\n",
      "| epoch   4 |    33/   35 batches | accuracy    0.269\n",
      "| epoch   5 |    11/   35 batches | accuracy    0.274\n",
      "| epoch   5 |    22/   35 batches | accuracy    0.307\n",
      "| epoch   5 |    33/   35 batches | accuracy    0.314\n",
      "| epoch   6 |    11/   35 batches | accuracy    0.326\n",
      "| epoch   6 |    22/   35 batches | accuracy    0.318\n",
      "| epoch   6 |    33/   35 batches | accuracy    0.307\n",
      "| epoch   7 |    11/   35 batches | accuracy    0.339\n",
      "| epoch   7 |    22/   35 batches | accuracy    0.362\n",
      "| epoch   7 |    33/   35 batches | accuracy    0.332\n",
      "| epoch   8 |    11/   35 batches | accuracy    0.361\n",
      "| epoch   8 |    22/   35 batches | accuracy    0.366\n",
      "| epoch   8 |    33/   35 batches | accuracy    0.357\n",
      "| epoch   9 |    11/   35 batches | accuracy    0.385\n",
      "| epoch   9 |    22/   35 batches | accuracy    0.393\n",
      "| epoch   9 |    33/   35 batches | accuracy    0.413\n",
      "| epoch  10 |    11/   35 batches | accuracy    0.416\n",
      "| epoch  10 |    22/   35 batches | accuracy    0.443\n",
      "| epoch  10 |    33/   35 batches | accuracy    0.417\n",
      "| epoch  11 |    11/   35 batches | accuracy    0.450\n",
      "| epoch  11 |    22/   35 batches | accuracy    0.463\n",
      "| epoch  11 |    33/   35 batches | accuracy    0.443\n",
      "| epoch  12 |    11/   35 batches | accuracy    0.480\n",
      "| epoch  12 |    22/   35 batches | accuracy    0.493\n",
      "| epoch  12 |    33/   35 batches | accuracy    0.475\n",
      "| epoch  13 |    11/   35 batches | accuracy    0.499\n",
      "| epoch  13 |    22/   35 batches | accuracy    0.519\n",
      "| epoch  13 |    33/   35 batches | accuracy    0.513\n",
      "| epoch  14 |    11/   35 batches | accuracy    0.534\n",
      "| epoch  14 |    22/   35 batches | accuracy    0.556\n",
      "| epoch  14 |    33/   35 batches | accuracy    0.547\n",
      "| epoch  15 |    11/   35 batches | accuracy    0.575\n",
      "| epoch  15 |    22/   35 batches | accuracy    0.572\n",
      "| epoch  15 |    33/   35 batches | accuracy    0.584\n",
      "| epoch  16 |    11/   35 batches | accuracy    0.624\n",
      "| epoch  16 |    22/   35 batches | accuracy    0.604\n",
      "| epoch  16 |    33/   35 batches | accuracy    0.608\n",
      "| epoch  17 |    11/   35 batches | accuracy    0.638\n",
      "| epoch  17 |    22/   35 batches | accuracy    0.640\n",
      "| epoch  17 |    33/   35 batches | accuracy    0.638\n",
      "| epoch  18 |    11/   35 batches | accuracy    0.670\n",
      "| epoch  18 |    22/   35 batches | accuracy    0.650\n",
      "| epoch  18 |    33/   35 batches | accuracy    0.620\n",
      "| epoch  19 |    11/   35 batches | accuracy    0.691\n",
      "| epoch  19 |    22/   35 batches | accuracy    0.674\n",
      "| epoch  19 |    33/   35 batches | accuracy    0.684\n",
      "| epoch  20 |    11/   35 batches | accuracy    0.705\n",
      "| epoch  20 |    22/   35 batches | accuracy    0.695\n",
      "| epoch  20 |    33/   35 batches | accuracy    0.704\n",
      "| epoch  21 |    11/   35 batches | accuracy    0.729\n",
      "| epoch  21 |    22/   35 batches | accuracy    0.721\n",
      "| epoch  21 |    33/   35 batches | accuracy    0.717\n",
      "| epoch  22 |    11/   35 batches | accuracy    0.733\n",
      "| epoch  22 |    22/   35 batches | accuracy    0.737\n",
      "| epoch  22 |    33/   35 batches | accuracy    0.734\n",
      "| epoch  23 |    11/   35 batches | accuracy    0.763\n",
      "| epoch  23 |    22/   35 batches | accuracy    0.739\n",
      "| epoch  23 |    33/   35 batches | accuracy    0.747\n",
      "| epoch  24 |    11/   35 batches | accuracy    0.781\n",
      "| epoch  24 |    22/   35 batches | accuracy    0.767\n",
      "| epoch  24 |    33/   35 batches | accuracy    0.785\n",
      "| epoch  25 |    11/   35 batches | accuracy    0.779\n",
      "| epoch  25 |    22/   35 batches | accuracy    0.775\n",
      "| epoch  25 |    33/   35 batches | accuracy    0.785\n",
      "| epoch  26 |    11/   35 batches | accuracy    0.803\n",
      "| epoch  26 |    22/   35 batches | accuracy    0.775\n",
      "| epoch  26 |    33/   35 batches | accuracy    0.797\n",
      "| epoch  27 |    11/   35 batches | accuracy    0.800\n",
      "| epoch  27 |    22/   35 batches | accuracy    0.820\n",
      "| epoch  27 |    33/   35 batches | accuracy    0.801\n",
      "| epoch  28 |    11/   35 batches | accuracy    0.825\n",
      "| epoch  28 |    22/   35 batches | accuracy    0.822\n",
      "| epoch  28 |    33/   35 batches | accuracy    0.825\n",
      "| epoch  29 |    11/   35 batches | accuracy    0.835\n",
      "| epoch  29 |    22/   35 batches | accuracy    0.840\n",
      "| epoch  29 |    33/   35 batches | accuracy    0.824\n",
      "| epoch  30 |    11/   35 batches | accuracy    0.846\n",
      "| epoch  30 |    22/   35 batches | accuracy    0.842\n",
      "| epoch  30 |    33/   35 batches | accuracy    0.833\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs+ 1):\n",
    "\n",
    "    # train\n",
    "    train(train_loader, model, optimizer, criterion, pred_func)\n",
    "    # evaluate\n",
    "    #train_metrics = evaluate(train_generator, model, criterion, pred_func)\n",
    "    # log history\n",
    "    #train_hist = append_metrics(train_hist, train_metrics)\n",
    "    # save model\n",
    "torch.save({'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()},\n",
    "        os.path.join('Model_save', name +'_'+ str(epoch) +'_7.pt'))\n",
    "    # update accuracy for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cb9cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6qmn', 1.4313704355117807e-07, 1.784120702737202e-12, 1.3535021992083784e-09, 3.048105838843185e-07, 2.2147023770457963e-08, 1.1026561423932435e-06, 1.9625827007985208e-07, 3.3099814000792094e-09, 0.9999974966049194, 4.5327566700059663e-10, 6.2931966304802245e-09, 1.551747619288335e-09, 7.931713445763489e-12, 7.176720941970416e-07, 8.163706155528416e-09, 7.367943061220572e-12, 1.2006333705016914e-09, 5.810391612293131e-10]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "def evaluate(dataloader, model, criterion, pred_func):\n",
    "    model.eval()\n",
    "    y_pred_proba = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tot_acc, tot_count = 0, 0\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            batch = batch.to(device)\n",
    "            _ , pred = pred_func(model, batch)\n",
    "            y_pred_proba.append(pred)\n",
    "    y_pred_proba = torch.cat(y_pred_proba, dim=0)  \n",
    "    y_pred_proba = torch.exp(y_pred_proba)\n",
    "    return y_pred_proba\n",
    "\n",
    "\n",
    "\n",
    "# Write predictions to a file\n",
    "test_dataloader = DataLoader(dataset=torch.LongTensor(X_test), batch_size=batch_size, drop_last=True)\n",
    "y_pred_proba = evaluate(test_dataloader, model, criterion, pred_func)[:-1]\n",
    "\n",
    "with open('sample_submission/sample_submission_CNN_BiLSTM_10.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = list()\n",
    "    for i in range(18):\n",
    "        lst.append('class'+str(i))\n",
    "    lst.insert(0, \"name\")\n",
    "    writer.writerow(lst)\n",
    "    for i, protein in enumerate(proteins_test):\n",
    "        lst = y_pred_proba[i,:].tolist()\n",
    "        lst.insert(0, protein)\n",
    "        writer.writerow(lst)\n",
    "    print(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9b6e0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1087, 18])\n"
     ]
    }
   ],
   "source": [
    "#print(y_pred_proba[0], torch.argmin(y_pred_proba[0]))\n",
    "\n",
    "args_max = y_pred_proba.argmax(1)\n",
    "r = F.one_hot(args_max, num_classes = 18)\n",
    "print(r.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db388e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pred = pd.read_csv('sample_submission_BiLSTM.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82543205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_pred_proba))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
