{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c01cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f7f4500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "a = sp.csr_matrix([[0, 1, 2, 0], \n",
    "                   [1, 1, 0, 0], \n",
    "                   [2, 0, 0, 3], \n",
    "                   [0, 0, 2, 3]])\n",
    "a_triu = len(np.where(a[np.triu_indices(a.shape[0])])[0]!=0)\n",
    "print(a.nnz)\n",
    "print(a_triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "704a7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nnz ne fonctionne pas correctement\n",
    "#need to take diago superior of nnz because adj symmetric\n",
    "\n",
    "def load_data(): \n",
    "    \"\"\"\n",
    "    Function that loads graphs\n",
    "    \"\"\"  \n",
    "    graph_indicator = np.loadtxt(\"data/PROT_graph_indicator.txt\", dtype=np.int64)\n",
    "    _, graph_size = np.unique(graph_indicator, return_counts=True)\n",
    "    \n",
    "    edges = np.loadtxt(\"data/edgelist.txt\", dtype=np.int64, delimiter=\",\")\n",
    "    A = sp.csr_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(graph_indicator.size, graph_indicator.size))\n",
    "    A += A.T\n",
    "    x = np.loadtxt(\"data/PROT_node_attributes.txt\", delimiter=\",\")\n",
    "    edge_attr = np.loadtxt(\"data/PROT_edge_attributes.txt\", delimiter=\",\")\n",
    "    \n",
    "    adj = []\n",
    "    features = []\n",
    "    edge_features = []\n",
    "    edge_per_graph = []\n",
    "    idx_n = 0\n",
    "    idx_m = 0\n",
    "    for i in range(graph_size.size):\n",
    "        adj.append(A[idx_n:idx_n+graph_size[i],idx_n:idx_n+graph_size[i]])\n",
    "        edge_a_triu = len(np.where(adj[i][np.triu_indices(adj[i].shape[0])])[0]!=0)\n",
    "        edge_features.append(edge_attr[idx_m:idx_m+edge_a_triu,:])\n",
    "        features.append(x[idx_n:idx_n+graph_size[i],:])\n",
    "        edge_per_graph.append(edge_a_triu)\n",
    "        idx_n += graph_size[i]\n",
    "        idx_m += edge_a_triu\n",
    "\n",
    "    return edge_per_graph, adj, features, edge_features\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    \"\"\"\n",
    "    Function that normalizes an adjacency matrix\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    A = A + sp.identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D = sp.diags(inv_degs)\n",
    "    A_normalized = D.dot(A)\n",
    "\n",
    "    return A_normalized\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"\n",
    "    Function that converts a Scipy sparse matrix to a sparse Torch tensor\n",
    "    \"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple message passing model that consists of 2 message passing layers\n",
    "    and the sum aggregation function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, n_class)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_in, adj, idx):\n",
    "        # first message passing layer\n",
    "        x = self.fc1(x_in)\n",
    "        x = self.relu(torch.mm(adj, x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # second message passing layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(torch.mm(adj, x))\n",
    "        \n",
    "        # sum aggregator\n",
    "        idx = idx.unsqueeze(1).repeat(1, x.size(1))\n",
    "        out = torch.zeros(torch.max(idx)+1, x.size(1)).to(x_in.device)\n",
    "        out = out.scatter_add_(0, idx, x)\n",
    "        \n",
    "        # batch normalization layer\n",
    "        out = self.bn(out)\n",
    "\n",
    "        # mlp to produce output\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc4(out)\n",
    "\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9f737dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graphs\n",
    "edge_per_graph, adj, features, edge_features = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1ce70e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('edge_per_graph.npy', edge_per_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa89457",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load graphs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m adj, features, edge_features \u001b[38;5;241m=\u001b[39m load_data() \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Normalize adjacency matrices\u001b[39;00m\n\u001b[1;32m      5\u001b[0m adj \u001b[38;5;241m=\u001b[39m [normalize_adjacency(A) \u001b[38;5;28;01mfor\u001b[39;00m A \u001b[38;5;129;01min\u001b[39;00m adj]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Load graphs\n",
    "adj, features, edge_features = load_data() \n",
    "\n",
    "# Normalize adjacency matrices\n",
    "adj = [normalize_adjacency(A) for A in adj]\n",
    "\n",
    "# Split data into training and test sets\n",
    "adj_train = list()\n",
    "features_train = list()\n",
    "y_train = list()\n",
    "adj_test = list()\n",
    "features_test = list()\n",
    "proteins_test = list()\n",
    "with open('graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            adj_test.append(adj[i])\n",
    "            features_test.append(features[i])\n",
    "        else:\n",
    "            adj_train.append(adj[i])\n",
    "            features_train.append(features[i])\n",
    "            y_train.append(int(t[1][:-1]))\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "n_hidden = 512\n",
    "n_input = 86\n",
    "dropout = 0.3\n",
    "learning_rate = 0.001\n",
    "n_class = 18\n",
    "\n",
    "# Compute number of training and test samples\n",
    "N_train = len(adj_train)\n",
    "N_test = len(adj_test)\n",
    "\n",
    "# Initializes model and optimizer\n",
    "model = GNN(n_input, n_hidden, dropout, n_class).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    # Iterate over the batches\n",
    "    for i in range(0, N_train, batch_size):\n",
    "        adj_batch = list()\n",
    "        features_batch = list()\n",
    "        idx_batch = list()\n",
    "        y_batch = list()\n",
    "        \n",
    "        # Create tensors\n",
    "        for j in range(i, min(N_train, i+batch_size)):\n",
    "            n = adj_train[j].shape[0]\n",
    "            adj_batch.append(adj_train[j]+sp.identity(n))\n",
    "            features_batch.append(features_train[j])\n",
    "            idx_batch.extend([j-i]*n)\n",
    "            y_batch.append(y_train[j])\n",
    "            \n",
    "        adj_batch = sp.block_diag(adj_batch)\n",
    "        features_batch = np.vstack(features_batch)\n",
    "\n",
    "        adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch).to(device)\n",
    "        features_batch = torch.FloatTensor(features_batch).to(device)\n",
    "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "        y_batch = torch.LongTensor(y_batch).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(features_batch, adj_batch, idx_batch)\n",
    "        loss = loss_function(output, y_batch)\n",
    "        train_loss += loss.item() * output.size(0)\n",
    "        count += output.size(0)\n",
    "        preds = output.max(1)[1].type_as(y_batch)\n",
    "        correct += torch.sum(preds.eq(y_batch).double())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(train_loss / count),\n",
    "              'acc_train: {:.4f}'.format(correct / count),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd996b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "y_pred_proba = list()\n",
    "# Iterate over the batches\n",
    "for i in range(0, N_test, batch_size):\n",
    "    adj_batch = list()\n",
    "    idx_batch = list()\n",
    "    features_batch = list()\n",
    "    y_batch = list()\n",
    "    \n",
    "    # Create tensors\n",
    "    for j in range(i, min(N_test, i+batch_size)):\n",
    "        n = adj_test[j].shape[0]\n",
    "        adj_batch.append(adj_test[j]+sp.identity(n))\n",
    "        features_batch.append(features_test[j])\n",
    "        idx_batch.extend([j-i]*n)\n",
    "        \n",
    "    adj_batch = sp.block_diag(adj_batch)\n",
    "    features_batch = np.vstack(features_batch)\n",
    "\n",
    "    adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch).to(device)\n",
    "    features_batch = torch.FloatTensor(features_batch).to(device)\n",
    "    idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "\n",
    "    output = model(features_batch, adj_batch, idx_batch)\n",
    "    y_pred_proba.append(output)\n",
    "    \n",
    "y_pred_proba = torch.cat(y_pred_proba, dim=0)\n",
    "y_pred_proba = torch.exp(y_pred_proba)\n",
    "y_pred_proba = y_pred_proba.detach().cpu().numpy()\n",
    "\n",
    "# Write predictions to a file\n",
    "with open('sample_submission.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = list()\n",
    "    for i in range(18):\n",
    "        lst.append('class'+str(i))\n",
    "    lst.insert(0, \"name\")\n",
    "    writer.writerow(lst)\n",
    "    for i, protein in enumerate(proteins_test):\n",
    "        lst = y_pred_proba[i,:].tolist()\n",
    "        lst.insert(0, protein)\n",
    "        writer.writerow(lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "044555d8adb5ccbd0ee428ac42afb0cd0f65da5ec357e768e8af7917df676d49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
