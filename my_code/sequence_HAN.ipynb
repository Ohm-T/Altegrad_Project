{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuVouapRmjEW"
   },
   "source": [
    "<center><h2>ALTeGraD 2022<br>Lab Session 1: HAN</h2><h3>Hierarchical Attention Network Using GRU</h3> 27 / 10 / 2022<br> M. Kamal Eddine, H. Abdine<br><br>\n",
    "\n",
    "\n",
    "<b>Student name:</b> Tom SALEMBIEN\n",
    "\n",
    "\n",
    "</center>\n",
    "In this lab, you will get familiar with recurrent neural networks (RNNs), self-attention, and the HAN architecture <b>(Yang et al. 2016)</b> using PyTorch. In this architecture, sentence embeddings are first individually produced, and a document embedding is then computed from the sentence embeddings.<br>\n",
    "<b>The deadline for this lab is November 14, 2022 11:59 PM.</b> More details about the submission and the architecture for this lab can be found in the handout PDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJaSJaIP1xRy"
   },
   "source": [
    "### = = = = =  Attention Layer = = = = =\n",
    "In this section, you will fill the gaps in the code to implement the self-attention layer. This layer will be used later to define the HAN architecture. The basic idea behind attention is that rather than considering the last annotation $h_T$ as a summary of the entire sequence, which is prone to information loss, the annotations at <i>all</i> time steps are used.\n",
    "The self-attention mechanism computes a weighted sum of the annotations, where the weights are determined by trainable parameters. Refer to <b>section 2.2</b> in the handout for the theoretical part, it will be needed to finish the first task.\n",
    "\n",
    "#### <b>Task 1:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 26 23:57:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    36W / 250W |    757MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    42W / 250W |  29925MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    89W / 250W |  29919MiB / 32510MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     68687      C   .../envs/notebook/bin/python      753MiB |\n",
      "|    1   N/A  N/A     59985      C   ...onda3/envs/jax/bin/python    29921MiB |\n",
      "|    2   N/A  N/A     60081      C   ...onda3/envs/jax/bin/python    29915MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yoM7H0KQncpF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionWithContext(nn.Module):\n",
    "    \"\"\"\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, return_coefficients=False, bias=True):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "        self.return_coefficients = return_coefficients\n",
    "\n",
    "        self.W = nn.Linear(input_shape, input_shape, bias=bias)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.u = nn.Linear(input_shape, 1, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.W.weight.data.uniform_(-initrange, initrange)\n",
    "        self.W.bias.data.uniform_(-initrange, initrange)\n",
    "        self.u.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # do not pass the mask to the next layers\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = (\n",
    "            mask.float()\n",
    "            .masked_fill(mask == 0, float(\"-inf\"))\n",
    "            .masked_fill(mask == 1, float(0.0))\n",
    "        )\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        #print(\"AttentionWithContext in :\", x.size())\n",
    "        uit = self.W(x)\n",
    "        uit = self.tanh(uit)\n",
    "        ait = self.u(uit)\n",
    "        a = torch.exp(ait)\n",
    "        \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            a = a*mask.double()\n",
    "        \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        eps = 1e-9\n",
    "        a = a / (torch.sum(a, axis=1, keepdim=True) + eps)\n",
    "        weighted_input = torch.sum(a*x, dim=1)\n",
    "        #print(\"AttentionWithContext out :\", weighted_input.size(), torch.sum(weighted_input, dim=1).size())\n",
    "        if self.return_coefficients:\n",
    "            return  [weighted_input, a] \n",
    "        else:\n",
    "            return  weighted_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgTP6GrOHlss"
   },
   "source": [
    "### = = = = = Parameters = = = = =\n",
    "In this section, we define the parameters to use in our training. Such as data path, the embedding dimention <b>d</b>, the GRU layer dimensionality <b>n_units</b>, etc..<br>\n",
    "The parameter <b>device</b> is used to train the model on GPU if it is available. for this purpose, if you are using Google Colab, switch your runtime to a GPU runtime to train the model with a maximum speed.<br>\n",
    "<b>Bonus question:</b> What is the purpose of the parameter <i>my_patience</i>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xviH2mrBFRug"
   },
   "source": [
    "<i>my_patience</i> parameter is used for early stopping, a regularization method to avoid overfitting. We stop the training if the performance of the model on the validation dataset starts to degrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czsVjxgYnczb",
    "outputId": "9ad28726-103c-467d-e852-eed995989a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "path_root = ''\n",
    "path_to_data = path_root + 'data/'\n",
    "\n",
    "d = 20 # dimensionality of amino acid embeddings\n",
    "n_units = 100 # RNN layer dimensionality\n",
    "drop_rate = 0.3 # dropout\n",
    "#input_size = (4888, 989, 20)\n",
    "input_size = (4888, 8466)\n",
    "padding_idx = 0\n",
    "oov_idx = 1\n",
    "batch_size = 32\n",
    "nb_epochs = 10\n",
    "my_patience = 2 # for early stopping strategy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device :\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8Vot_C7Hlst"
   },
   "source": [
    "### = = = = = Data Loading = = = = =\n",
    "In this section we will use first <b>wget</b> to download the data the we will load it using numpy in the first cell. While in the second cell, we will use these data to define our Pytorch data loader. Note that the data is already preprocessed, tokenized and padded.<br><br>\n",
    "<b>Note: if you are running your notebook on Windows or on MacOS, <i>wget</i> will probably not work if you did not install it manually. In this case, use the provided link to download the data and change the <i>path_to_data</i> in the <i>Parameters</i> section accordingly. Otherwise, you will face no problem on Ubuntu and Google Colab.</b>\n",
    "\n",
    "#### <b>Task 2.1:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files for ohe\n",
    "graph_indicator = np.loadtxt(\"graph_indicator.txt\", dtype=np.int64)\n",
    "nodes = np.loadtxt(\"node_attributes.txt\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68064/1234997268.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643003845/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  ohe_vec = torch.Tensor([node[3:23] for node in nodes[np.where(graph_indicator==i)]])\n"
     ]
    }
   ],
   "source": [
    "# Read sequences\n",
    "sequences = list()\n",
    "with open('sequences.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        sequences.append(line[:-1])\n",
    "\n",
    "# Split data into training and test sets\n",
    "sequences_train = list()\n",
    "sequences_test = list()\n",
    "train_ohe = list()\n",
    "test_ohe = list()\n",
    "proteins_test = list()\n",
    "y_train = list()\n",
    "with open('graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        ohe_vec = torch.Tensor([node[3:23] for node in nodes[np.where(graph_indicator==i)]])\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            sequences_test.append(sequences[i])\n",
    "            test_ohe.append(ohe_vec)\n",
    "            \n",
    "        else:\n",
    "            sequences_train.append(sequences[i])\n",
    "            y_train.append(int(t[1][:-1]))\n",
    "            train_ohe.append(ohe_vec)\n",
    "\n",
    "\n",
    "\n",
    "train_ohe = nn.utils.rnn.pad_sequence(train_ohe).permute(1, 0, 2).long()\n",
    "test_ohe = nn.utils.rnn.pad_sequence(test_ohe).permute(1, 0, 2).long()\n",
    "pad_ = (0, 0, 0, 79)\n",
    "test_ohe = F.pad(test_ohe, pad_, \"constant\", 0)\n",
    "y_train = F.one_hot(torch.Tensor(y_train).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4888, 989, 20])\n",
      "torch.Size([1223, 989, 20])\n",
      "torch.Size([4888, 18])\n"
     ]
    }
   ],
   "source": [
    "print(train_ohe.size())\n",
    "print(test_ohe.size())\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4686\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique([x[:9] for x in sequences_train])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "def create_dicts(sequence=amino_acids):\n",
    "    \"\"\"\n",
    "    Create the dicts for the sequence embedding\n",
    "    \"\"\"\n",
    "    word_to_index = dict(zip(sequence, range(1,21)))\n",
    "    # invert mapping\n",
    "    index_to_word =  {v : k for k, v in word_to_index.items()}\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "word_to_index, index_to_word = create_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4888, 8466) (1223, 8466) torch.Size([4888, 18])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"\n",
    "    Function that converts a Scipy sparse matrix to a sparse Torch tensor\n",
    "    \"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.int_)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "# Read sequences\n",
    "sequences = list()\n",
    "with open('sequences.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        sequences.append(line[:-1])\n",
    "\n",
    "# Split data into training and test sets\n",
    "sequences_train = list()\n",
    "sequences_test = list()\n",
    "proteins_test = list()\n",
    "y_train = list()\n",
    "with open('graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            sequences_test.append(sequences[i])\n",
    "        else:\n",
    "            sequences_train.append(sequences[i])\n",
    "            y_train.append(int(t[1][:-1]))\n",
    "\n",
    "# Map sequences to \n",
    "vec = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "X_train = vec.fit_transform(sequences_train).todense()\n",
    "X_test = vec.transform(sequences_test).todense()\n",
    "y_train = F.one_hot(torch.Tensor(y_train).long())\n",
    "print(X_train.shape, X_test.shape, y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OHE TESTING WITHOUT FILES NODE ATRIBUTE AND GRAPH_INDICATOR ########\n",
    "\n",
    "# def encode_pad(dataset, word_to_index = word_to_index):\n",
    "#     \"\"\"\n",
    "#     Encoding and padding of the amino acids\n",
    "#     \"\"\"\n",
    "#     encode = []\n",
    "#     for row in dataset:\n",
    "#         row_encode = []\n",
    "#         for aa in row:\n",
    "#             row_encode.append(word_to_index.get(aa))\n",
    "#         encode.append(torch.Tensor(row_encode))\n",
    "    \n",
    "#     # Padding\n",
    "#     encode.append(torch.ones(989))\n",
    "#     encode = nn.utils.rnn.pad_sequence(encode)\n",
    "#     encode = torch.transpose(encode, 0, 1)\n",
    "#     return encode[:-1]\n",
    "\n",
    "# train_encode = encode_pad(sequences_train).long()\n",
    "# test_encode = encode_pad(sequences_test).long()\n",
    "\n",
    "# print(\"Size : nbreview*nb_sentence*sent_size\")\n",
    "# sizes = [train_encode, y_train, test_encode, proteins_test]\n",
    "# for x in sizes:\n",
    "#     print(np.shape(x), np.shape(x[0]))\n",
    "    \n",
    "# train_ohe = F.one_hot(train_encode)\n",
    "# test_ohe = F.one_hot(test_encode)\n",
    "\n",
    "# print(train_ohe.size(), test_ohe.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DpsCvmaiJfZc"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class Dataset_(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.documents = x\n",
    "        self.labels = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        document = self.documents[index]\n",
    "        label = self.labels[index] \n",
    "        sample = {\n",
    "            \"document\": torch.tensor(document),\n",
    "            \"label\": torch.tensor(label),\n",
    "            }\n",
    "        return sample\n",
    "\n",
    "\n",
    "def get_loader(x, y, batch_size=32):\n",
    "    dataset = Dataset_(x, y)\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True,\n",
    "                            )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rzqEGOdHlst"
   },
   "source": [
    "### = = = = = Defining Architecture = = = = =\n",
    "In this section, we define the HAN architecture. We start with <i>AttentionBiGRU</i> module in order to define the sentence encoder (check Figure 3 in the handout). Then, we define the <i>TimeDistributed</i> module to allow us to forward our input (batch of document) as to the sentence encoder as <b>batch of sentences</b>, where each sentence in the document will be considered as a time step. This module also reshape the output to a batch of timesteps representations per document. Finally we define the <b>HAN</b> architecture using <i>TimeDistributed</i>, <i>AttentionWithContext</i> and <i>GRU</i>.\n",
    "\n",
    "#### <b>Task 2.2:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "AMj9j1_pHlst"
   },
   "outputs": [],
   "source": [
    "\n",
    "class AttentionBiGRU(nn.Module):\n",
    "    def __init__(self, input_shape, n_units, index_to_word, dropout=drop_rate):\n",
    "        super(AttentionBiGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(index_to_word)+2,# fill the gap # vocab size\n",
    "                                      d, # dimensionality of embedding space\n",
    "                                      padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(input_size=d,\n",
    "                          hidden_size=n_units,\n",
    "                          num_layers=1,\n",
    "                          bias=True,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True)\n",
    "        self.attention = AttentionWithContext(2*n_units,   # fill the gap # the input shape for the attention layer\n",
    "                                              return_coefficients=True)\n",
    "\n",
    "\n",
    "    def forward(self, sent_ints):\n",
    "        #print(\"AttentionBiGru in :\", sent_ints.size())\n",
    "        sent_wv = self.embedding(sent_ints)\n",
    "        #print(sent_wv.size())\n",
    "        sent_wv_dr = self.dropout(sent_wv)\n",
    "        sent_wa, _ =  self.gru(sent_wv_dr)# fill the gap # RNN layer\n",
    "        sent_att_vec, word_att_coeffs = self.attention(sent_wa) # fill the gap # attentional vector for the sent\n",
    "        sent_att_vec_dr = self.dropout(sent_att_vec)  \n",
    "        #print(\"AttentionBiGru out :\", sent_att_vec_dr.size())   \n",
    "        return sent_att_vec_dr, word_att_coeffs\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size) (448, 30)\n",
    "        #print(\"Time distributed\", x_reshape.size())\n",
    "        sent_att_vec_dr, word_att_coeffs = self.module(x_reshape)\n",
    "        # We have to reshape the output\n",
    "        if self.batch_first:\n",
    "            sent_att_vec_dr = sent_att_vec_dr.contiguous().view(x.size(0), -1, sent_att_vec_dr.size(-1))  # (samples, timesteps, output_size)\n",
    "            word_att_coeffs = word_att_coeffs.contiguous().view(x.size(0), -1, word_att_coeffs.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            sent_att_vec_dr = sent_att_vec_dr.view(-1, x.size(1), sent_att_vec_dr.size(-1))  # (timesteps, samples, output_size)\n",
    "            word_att_coeffs = word_att_coeffs.view(-1, x.size(1), word_att_coeffs.size(-1))  # (timesteps, samples, output_size)\n",
    "        return sent_att_vec_dr, word_att_coeffs      \n",
    "\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, input_shape, n_units, index_to_word, dropout=0):\n",
    "        super(HAN, self).__init__()\n",
    "        self.encoder = AttentionBiGRU(input_shape, n_units, index_to_word, dropout)\n",
    "        self.timeDistributed = TimeDistributed(self.encoder, True)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.gru = nn.GRU(input_size=2*n_units,# fill the gap # the input shape of GRU layer\n",
    "                          hidden_size=n_units,\n",
    "                          num_layers=1,\n",
    "                          bias=True,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True)\n",
    "        self.attention = AttentionWithContext(2*n_units, # fill the gap # the input shape of between-sentence attention layer\n",
    "                                              return_coefficients=True)\n",
    "        self.lin_out = nn.Linear(2*n_units,   # fill the gap # the input size of the last linear layer\n",
    "                                 18)\n",
    "        self.preds = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, doc_ints):\n",
    "        #print('HAN to time distrib', doc_ints.size())\n",
    "        sent_att_vecs_dr, word_att_coeffs = self.timeDistributed(doc_ints.to(device).long())\n",
    "        #print('Time Distrib to gru', sent_att_vecs_dr.size())\n",
    "        doc_sa, _ = self.gru(sent_att_vecs_dr)\n",
    "        #print('GRU to attention', doc_sa.size())\n",
    "        doc_att_vec, sent_att_coeffs = self.attention(doc_sa)\n",
    "        #print('Attention to lin_out', doc_att_vec.size())\n",
    "        doc_att_vec_dr = self.dropout(doc_att_vec)\n",
    "        doc_att_vec_dr = self.lin_out(doc_att_vec_dr)\n",
    "        #print(\"lin_out\", doc_att_vec_dr.size())\n",
    "        return self.preds(doc_att_vec_dr), word_att_coeffs, sent_att_coeffs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgreR5AcHlst"
   },
   "source": [
    "### = = = = = Training = = = = =\n",
    "In this section, we have two code cells. In the first one, we define our evaluation function to compute the training and validation accuracies. While in the second one, we define our model, loss and optimizer and train the model over <i>nb_epochs</i>.<br>\n",
    "<b>Bonus task:</b> use <a href=\"https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html\" target=\"_blank\">tensorboard</a> to visualize the loss and the validation accuray during the training.\n",
    "\n",
    "#### <b>Task 2.3:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRYiKhZEEidb",
    "outputId": "0832ba89-87ea-4a0a-fa31-da1277b9ace8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/152 [00:00<?, ?batch/s]/tmp/ipykernel_105465/320991302.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"label\": torch.tensor(label),\n",
      "Epoch 1: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 152/152 [01:50<00:00,  1.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 1 Complete: Avg. Loss: 2.5846\n",
      "Train Loss improved, saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 152/152 [01:50<00:00,  1.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 2 Complete: Avg. Loss: 2.5474\n",
      "Train Loss improved, saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 152/152 [01:50<00:00,  1.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 3 Complete: Avg. Loss: 2.5471\n",
      "Train Loss improved, saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 152/152 [01:50<00:00,  1.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 4 Complete: Avg. Loss: 2.5477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 152/152 [01:50<00:00,  1.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 5 Complete: Avg. Loss: 2.5467\n",
      "Train Loss improved, saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 152/152 [01:50<00:00,  1.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 6 Complete: Avg. Loss: 2.5469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 152/152 [01:50<00:00,  1.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 7 Complete: Avg. Loss: 2.5461\n",
      "Train Loss improved, saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 152/152 [01:50<00:00,  1.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 8 Complete: Avg. Loss: 2.5467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 152/152 [01:50<00:00,  1.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 9 Complete: Avg. Loss: 2.5476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 152/152 [01:50<00:00,  1.38batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 10 Complete: Avg. Loss: 2.5460\n",
      "Train Loss improved, saving model...\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model = HAN(input_size, n_units, index_to_word).to(device)\n",
    "model = model.double()\n",
    "lr = 0.001  # learning rate\n",
    "criterion = nn.CrossEntropyLoss()# fill the gap, use Binary cross entropy from torch.nn: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) #fill me\n",
    "\n",
    "def train(x_train=X_train,\n",
    "          y_train=y_train,\n",
    "          x_test=X_test,\n",
    "          word_dict=index_to_word,\n",
    "          batch_size=batch_size):\n",
    "  \n",
    "    train_data = get_loader(x_train, y_train, batch_size)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    p = 0 # patience\n",
    "\n",
    "    for epoch in range(1, nb_epochs + 1): \n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        with tqdm(train_data, unit=\"batch\") as tepoch:\n",
    "            for idx, data in enumerate(tepoch):\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                input = data['document'].to(device)\n",
    "                label = data['label'].to(device)\n",
    "                label = label.double()\n",
    "                output = model.forward(input)[0]\n",
    "                loss = criterion(output, label) # fill the gap # compute the loss\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient \n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\"\n",
    "              .format(epoch, sum(losses)/len(losses)))\n",
    "        train_loss = sum(losses)/len(losses)\n",
    "        if train_loss <= best_loss:\n",
    "            best_loss = train_loss\n",
    "            print(\"Train Loss improved, saving model...\")\n",
    "            torch.save(model.state_dict(), './best_model.pt')\n",
    "            p = 0\n",
    "#         else:\n",
    "#             p += 1\n",
    "#             if p==my_patience:\n",
    "#                 print(\"Validation accuracy did not improve for {} epochs, stopping training...\".format(my_patience))\n",
    "#     print(\"Loading best checkpoint...\")    \n",
    "#     model.load_state_dict(torch.load('./best_model.pt'))\n",
    "#     model.eval()\n",
    "    print('done.')\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HAN(\n",
       "  (encoder): AttentionBiGRU(\n",
       "    (embedding): Embedding(22, 20, padding_idx=0)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (gru): GRU(20, 100, batch_first=True, bidirectional=True)\n",
       "    (attention): AttentionWithContext(\n",
       "      (W): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (tanh): Tanh()\n",
       "      (u): Linear(in_features=200, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (timeDistributed): TimeDistributed(\n",
       "    (module): AttentionBiGRU(\n",
       "      (embedding): Embedding(22, 20, padding_idx=0)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "      (gru): GRU(20, 100, batch_first=True, bidirectional=True)\n",
       "      (attention): AttentionWithContext(\n",
       "        (W): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (tanh): Tanh()\n",
       "        (u): Linear(in_features=200, out_features=1, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (gru): GRU(200, 100, batch_first=True, bidirectional=True)\n",
       "  (attention): AttentionWithContext(\n",
       "    (W): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (tanh): Tanh()\n",
       "    (u): Linear(in_features=200, out_features=1, bias=False)\n",
       "  )\n",
       "  (lin_out): Linear(in_features=200, out_features=18, bias=True)\n",
       "  (preds): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./best_model.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dataloader = DataLoader(dataset=X_test,\n",
    "                            batch_size=1,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=True,\n",
    "                            )\n",
    "    y_pred = torch.zeros(1,18).to(device)\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        y_pred = torch.cat([y_pred, model(data.to(device))[0]], 0)\n",
    "    y_pred = y_pred[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Write predictions to a file\n",
    "with open('sample_submission_han.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = list()\n",
    "    for i in range(18):\n",
    "        lst.append('class'+str(i))\n",
    "    lst.insert(0, \"name\")\n",
    "    writer.writerow(lst)\n",
    "    for i, protein in enumerate(proteins_test):\n",
    "        lst = y_pred[i].tolist()\n",
    "        lst.insert(0, protein)\n",
    "        writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "497396fc279c3f97c33f1484ac27517a9d57f6ae267a9924b25221b965319414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
